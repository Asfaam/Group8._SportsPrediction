# -*- coding: utf-8 -*-
"""Group8._SportsPrediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Yn9TZ8HE9PdXTaZ0NNygp50bJyLLCHny

**Title**: *Sports Prediction- FIFA Player Rating Prediction*

**Description:** *This code will build a machine learning model for  predicting
                  a player's overall rating and confidence based on the profile and then deploy the best model as a web application.*

**Author:** *Faisal Alidu, Emmanuel Soumahoro*

**Date:** *22nd October 2023*
"""

from google.colab import drive
drive.mount('/content/drive')

# Import libraries
import pickle
import warnings
import numpy as np
import pandas as pd
from sklearn.svm import SVR
from sklearn.svm import SVC
import matplotlib.pyplot as plt
from xgboost import XGBRegressor
from sklearn import tree, metrics
from sklearn.impute import SimpleImputer
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score
from sklearn.ensemble import VotingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import GridSearchCV
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import make_scorer, mean_absolute_error, mean_squared_error, r2_score
# Suppress warnings
warnings.simplefilter(action='ignore', category=Warning)

"""### Data Preparation & Feature Extraction"""

# Data Loading

pd.set_option('display.max_columns', None)

players_21 = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Intro to AI Folder/Mid-Semester Project/players_21.csv')

players_22 = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Intro to AI Folder/Mid-Semester Project/players_22.csv')

players_21.head()

players_22.head()

players_21.shape

players_22.shape

players_21.info()

players_22.info()

# The describe function gives information about only numerical columns.

players_21.describe().T.style.background_gradient(cmap='YlGn')

# The describe function gives information about only numerical columns.

players_22.describe().T.style.background_gradient(cmap='inferno')

players_21.hist(bins=40, figsize=(27,17))
plt.show()

players_22.hist(bins=40, figsize=(27,17))
plt.show()

### Rename the column: overall to overall_rating
players_21.rename(columns={'overall':'overall_rating', }, inplace=True)

players_22.rename(columns={'overall':'overall_rating', }, inplace=True)

# select and keep only the columns in where the proportion of missing values is less than or equal to 0.3 (30%)
# for players_21
players_21 = players_21.loc[:, players_21.isnull().mean() <= .3]
# for players_22
players_22 = players_22.loc[:, players_22.isnull().mean() <= .3]

players_21.shape

players_22.shape

### Check for null values.

# For players_21
nan_values_21 = players_21.isna()
nan_columns_21 = nan_values_21.any()

# For players_22
nan_values_22 = players_22.isna()
nan_columns_22 = nan_values_22.any()

print("\nplayers_21:\n")
print(nan_columns_21,"\n")
print("\nplayers_22:\n")
print(nan_columns_22)

# Print the columns names containing null values or missing values in the form of a list

# For players_21
missing_values_21 = players_21.isnull().sum()
columns_with_missing_values_21 = missing_values_21[missing_values_21 > 0].index.tolist()

# For players_22
missing_values_22 = players_22.isnull().sum()
# Check for columns with more than 30 missing values
columns_with_missing_values_22 = missing_values_22[missing_values_22 > 0].index.tolist()

print(columns_with_missing_values_21,"\n")
print(columns_with_missing_values_22)

# Extract Categorical features

categorical_cols_21 = players_21.select_dtypes(include=['object']).columns
categorical_cols_22 = players_22.select_dtypes(include=['object']).columns
print(categorical_cols_21)
print(categorical_cols_22)

# Categorical features: Forward fill missing values;

# for players_21
players_21[categorical_cols_21].fillna(method='ffill', inplace=True)

# for players_22
players_22[categorical_cols_22].fillna(method='ffill', inplace=True)

players_21[categorical_cols_21].head()

# For players_21
nan_values_21 = players_21[categorical_cols_21].isna()
nan_columns_21 = nan_values_21.any()

missing_values_21 = players_21[categorical_cols_21].isnull().sum()
columns_with_missing_values_21 = missing_values_21[missing_values_21 > 0].index.tolist()

print(missing_values_21)
print(columns_with_missing_values_21)

## Keep the following categorical features for players_21

preferred_foot_21 = players_21['preferred_foot']
work_rate_21 = players_21['work_rate']

## Keep the following categorical features for players_22

preferred_foot_22 = players_22['preferred_foot']
work_rate_22 = players_22['work_rate']

## Eliminating columns: dropping the categorical features

players_21.drop(categorical_cols_21, axis=1, inplace=True)
players_22.drop(categorical_cols_22, axis=1, inplace=True)

# dropping sofifa_id
player_21 = players_21.drop(['sofifa_id'], axis=1, inplace=True )
player_22 = players_22.drop(['sofifa_id'], axis=1, inplace=True )

## Convert categorical features into numeric for players_21
preferred_foot_21 = pd.get_dummies(preferred_foot_21, prefix='preferred_foot')
work_rate_21 = pd.get_dummies(work_rate_21, prefix='work_rate')

## Convert categorical features into numeric for players_22
preferred_foot_22 = pd.get_dummies(preferred_foot_22, prefix='preferred_foot')
work_rate_22 = pd.get_dummies(work_rate_22, prefix='work_rate')

work_rate_22

preferred_foot_22

## At this point players_21 and players_22 only have numeric features in the dataset. Lets impute the missing values.

# for players_21
imp = SimpleImputer()
imputed_data_21 = imp.fit_transform(players_21)
players_21 = pd.DataFrame(imputed_data_21, columns=players_21.columns)


# for players_22
imp = SimpleImputer()
imputed_data_22 = imp.fit_transform(players_22)
players_22 = pd.DataFrame(imputed_data_22, columns=players_22.columns)

# Join both transformed numeric and non-numeric columns to form fully numeric columns
players_21 = pd.concat([players_21, preferred_foot_21, work_rate_21,], axis=1)

players_22 = pd.concat([players_22, preferred_foot_22, work_rate_22], axis=1)

players_21.info()

players_22.info()

players_21.shape

players_22.shape

### Check for null values again

# For players_21
nan_values_21 = players_21.isna()
nan_columns_21 = nan_values_21.any()

# For players_22
nan_values_22 = players_22.isna()
nan_columns_22 = nan_values_22.any()

print("\nplayers_21:\n")
print(nan_columns_21,"\n")
print("\nplayers_22:\n")
print(nan_columns_22)

# Again print the columns names containing null values or missing values in the form of a list

# For players_21
missing_values_21 = players_21.isnull().sum()
columns_with_missing_values_21 = missing_values_21[missing_values_21 > 0].index.tolist()

# For players_22
missing_values_22 = players_22.isnull().sum()
columns_with_missing_values_22 = missing_values_22[missing_values_22 > 0].index.tolist()


print(columns_with_missing_values_21)
print(columns_with_missing_values_22)

"""### Feature Selection"""

# Calculate the correlation between each feature and the target variable
correlation_matrix = players_21.corr()
target_correlation = correlation_matrix['overall_rating'].drop('overall_rating')

# Select the top-k features with the highest absolute correlation
k = 20
top_k_features = target_correlation[target_correlation > 0.5].abs().nlargest(k).index

# Collect the column names of top_k_features in a list and store it in the 'selected_features' variable
selected_features = top_k_features.tolist()

# Display the selected features and their correlation with the target variable
print(f"\nTop {k} features with maximum correlation with the target variable:\n")
print(players_21[top_k_features])


#The correlation matrix produces the same results for players_22
'''
# Calculate the correlation between each feature and the target variable
correlation_matrix = players_22.corr()
target_correlation = correlation_matrix['overall_rating'].drop('overall_rating')

# Select the top-k features with the highest absolute correlation
k = 20
top_k_features = target_correlation[target_correlation > 0.5].abs().nlargest(k).index

# Collect the column names of top_k_features in a list and store it in the 'selected_features' variable
selected_features = top_k_features.tolist()

# Display the selected features and their correlation with the target variable
print(f"\nTop {k} features with maximum correlation with the target variable:\n")
print(players_21[top_k_features])

print(selected_features)
'''

# List of features that would be trained
selected_features

# Select the the independent (X) and dependent (Y) variables from the DataFrame
X = players_21[selected_features]
y = players_21['overall_rating']

X.head()

y.head()

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
X = pd.DataFrame(X_scaled, columns=X.columns)

"""### Model Creation and Training"""

# Split the data into training and testing sets
x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Model 1 : Support Vector Regression
svr_model = SVR(kernel='linear')  # You can choose the kernel you prefer
svr_model.fit(x_train, y_train)
y_pred = svr_model.predict(x_test)

# Evaluate the  model
mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print('\nAccuracy: {}'.format(svr_model.score(x_test, y_test)))
print("mean_absolute_error:", mae)
print("mean_squared_error:", mse)
print("r2_score:", r2)

# Model 2 : Naive Bayes Algorithm

nb_model = GaussianNB()
nb_model.fit(x_train, y_train)

y_pred = nb_model.predict(x_test)

# Evaluate the model
mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print('\nAccuracy: {}'.format(nb_model.score(x_test, y_test)))
print("mean_absolute_error:",mae)
print("mean_squared_error:",mse)
print("r2_score:",r2)
print("\nconfusion_matrix:\n",confusion_matrix(y_test, y_pred))
print("\n\nclassification_report:\n",classification_report(y_test, y_pred))

"""Ensemble Models"""

# Model 3: Train RandomForest, XGBoost and Gradient Boost Regressors models with cv and grid search:
models = {
    'RandomForest': RandomForestRegressor(),
    'XGBoost': XGBRegressor(),
    'GradientBoost': GradientBoostingRegressor()
}

params = {
    'RandomForest': {'n_estimators': [4, 5], 'max_depth': [None, 5]},
    'XGBoost': {'n_estimators': [5, 6], 'learning_rate': [0.01, 0.1]},
    'GradientBoost': {'n_estimators': [4, 6], 'learning_rate': [0.01, 0.1]}
}

for name, model in models.items():
    gs = GridSearchCV(model, params[name], cv=5)
    gs.fit(x_train, y_train)
    # Make predictions using the best model from GridSearchCV
    y_pred = gs.predict(x_test)

    # Evaluate the model
    mae = mean_absolute_error(y_test, y_pred)
    mse = mean_squared_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)

    print(f"\nBest parameters for {name}: {gs.best_params_}")
    print(f"Validation score for {name}: {gs.score(x_test, y_test)}")
    print(f"mean_absolute_error for {name}: {mae}")
    print(f"mean_squared_error for {name}: {mse}",)
    print(f"r2_score for {name}: {r2}\n")

'''
Mean Squared Error (MSE) is a common metric for regression problems, and it measures the average of the squared differences between the predicted values and the actual values.
A lower MSE indicates that the model's predictions are, on average, closer to the true values.  Lower MSE often corresponds to better model performance.
'''

rf_model = RandomForestRegressor(
    n_estimators=100,
    max_depth=None,
    min_samples_split=2,
    min_samples_leaf=1,
    max_features='auto',
    random_state=42
)

# Train the model
rf_model.fit(x_train, y_train)
y_pred = gs.predict(x_test)

# Evaluate the model
mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print("mean_absolute_error:",mae)
print("mean_squared_error:",mse)
print("r2_score:",r2)

# Model 4: VotingClassifier

decision_tree = DecisionTreeClassifier(random_state=42, criterion='entropy')
knn = KNeighborsClassifier(n_neighbors=8)
svm = SVC(probability=True, random_state=42)

voting_classifier = VotingClassifier(estimators=[
    ('decision_tree', decision_tree),
    ('knn', knn),
    ('svm', svm)
], voting='soft')

for model in (decision_tree, knn, svm,voting_classifier):
  model.fit(x_train,y_train)
  y_pred=model.predict(x_test)
  print(model.__class__.__name__,accuracy_score(y_pred,y_test))

# Model 5: RandomForestClassifier

rfc=RandomForestClassifier(n_estimators=20, max_depth=3, criterion='entropy')

# Perform cross-validation
cv_scores = cross_val_score(rfc, x_train, y_train, cv=5)
print(f"\nCross-validation scores: {cv_scores}")
print(f"\nMean cross-validation score: {cv_scores.mean()}")

# Fit the model
rfc.fit(x_train,y_train)
y_pred=rfc.predict(x_test)
accuracy_score(y_pred,y_test)
print('\nAccuracy of the model:',accuracy_score(y_pred,y_test))

# Evaluate the model
mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
print("\nmean_absolute_error of the model:",mae)
print("\nmean_squared_error of the model:",mse)
print("\nr2_score of the model:",r2)

# Fine-tune the model (RandomForestClassifier) with GridSearchCV
n_estimators_range = list(range(1, 31))

# Create a parameter grid: map the parameter names to the values that should be searched
param_grid = dict(n_estimators=n_estimators_range)

grid = GridSearchCV(RandomForestClassifier(max_depth=3, criterion='entropy'), param_grid, cv=10, scoring='accuracy')
grid.fit(x_train, y_train)

rfc=RandomForestClassifier(n_estimators=grid.best_params_['n_estimators'], max_depth=3, criterion='entropy')
rfc.fit(x_train,y_train)
y_pred=rfc.predict(x_test)
accuracy_score(y_pred,y_test)

# Evaluate the model
mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

# Examine the best model
print("\ngrid.best_score:",grid.best_score_)
print("\ngrid.best_params:",grid.best_params_)
print("\ngrid.best_estimator:",grid.best_estimator_)
print("\nAccuracy of the best model:", accuracy_score(y_pred,y_test))
print("\nmean_absolute_error of the best model:",mae)
print("\nmean_squared_error of the best model:",mse)
print("\nr2_score of the best model:",r2)

# correlation between variables in selected_features and the target variable; overall_rating
for name, score in zip(x_train.columns, rfc.feature_importances_):
  print(name, score)

"""### Testing the models using players_22"""

x_test_22 = players_22[selected_features]
y_test_22 = players_22['overall_rating']

scaler = StandardScaler()
x_test_22_scaled = scaler.fit_transform(x_test_22)
x_test_22 = pd.DataFrame(x_test_22_scaled, columns=x_test_22.columns)

1# testing players_22 on Support Vector Regression
svr_model = SVR(kernel='linear')  # You can choose the kernel you prefer
svr_model.fit(x_train, y_train)
y_pred = svr_model.predict(x_test)

# Evaluate the  model
mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print('\nAccuracy: {}'.format(svr_model.score(x_test, y_test)))
print("mean_absolute_error:", mae)
print("mean_squared_error:", mse)
print("r2_score:", r2)

2# testing players_22 on Naive Bayes Algorithm

nb_model = GaussianNB()
nb_model.fit(x_train, y_train)

y_pred = nb_model.predict(x_test_22)

# Evaluate the model
mae = mean_absolute_error(y_test_22, y_pred)
mse = mean_squared_error(y_test_22, y_pred)
r2 = r2_score(y_test_22, y_pred)

print('\nAccuracy: {}'.format(nb_model.score(x_test_22, y_test_22)))
print("mean_absolute_error:",mae)
print("mean_squared_error:",mse)
print("r2_score:",r2)
print("\nconfusion_matrix:\n",confusion_matrix(y_test_22, y_pred))
print("\n\nclassification_report:\n",classification_report(y_test_22, y_pred))

3# testing players_22 on RandomForest, XGBoost, Gradient Boost Regressors
models = {
    'RandomForest': RandomForestRegressor(),
    'XGBoost': XGBRegressor(),
    'GradientBoost': GradientBoostingRegressor()
}

params = {
    'RandomForest': {'n_estimators': [4, 5], 'max_depth': [None, 5]},
    'XGBoost': {'n_estimators': [5, 6], 'learning_rate': [0.01, 0.1]},
    'GradientBoost': {'n_estimators': [4, 6], 'learning_rate': [0.01, 0.1]}
}

for name, m in models.items():
    gs = GridSearchCV(m, params[name], cv=5)
    gs.fit(x_train, y_train)
    # Make predictions using the best model from GridSearchCV
    y_pred = gs.predict(x_test_22)

    # Evaluate the model
    mae = mean_absolute_error(y_test_22, y_pred)
    mse = mean_squared_error(y_test_22, y_pred)
    r2 = r2_score(y_test_22, y_pred)

    #print('\nAccuracy: {}'.format(nb_model.score(x_test, y_test)))
    print(f"\nBest parameters for {name}: {gs.best_params_}")
    print(f"Validation score for {name}: {gs.score(x_test_22, y_test_22)}")
    #print()
    print(f"mean_absolute_error for {name}: {mae}")
    print(f"mean_squared_error for {name}: {mse}",)
    print(f"r2_score for {name}: {r2}\n")

4# testing players_22 on VotingClassifier

decision_tree = DecisionTreeClassifier(random_state=42, criterion='entropy')
knn = KNeighborsClassifier(n_neighbors=8)
svm = SVC(probability=True, random_state=42)

voting_classifier = VotingClassifier(estimators=[
    ('decision_tree', decision_tree),
    ('knn', knn),
    ('svm', svm)
], voting='soft')

for model in (decision_tree, knn, svm,voting_classifier):
  model.fit(x_train,y_train)
  y_pred=model.predict(x_test_22)
  print(model.__class__.__name__,accuracy_score(y_pred,y_test_22))

5# testing players_22 on RandomForestClassifier

rfc=RandomForestClassifier(n_estimators=20, max_depth=3, criterion='entropy')

# Perform cross-validation
cv_scores = cross_val_score(rfc, x_train, y_train, cv=5)
print(f"\nCross-validation scores: {cv_scores}")
print(f"\nMean cross-validation score: {cv_scores.mean()}")

# Fit the model
rfc.fit(x_train,y_train)
y_pred=rfc.predict(x_test_22)
accuracy_score(y_pred,y_test_22)
print('\nAccuracy of the model:',accuracy_score(y_pred,y_test_22))

# Evaluate the model
mae = mean_absolute_error(y_test_22, y_pred)
mse = mean_squared_error(y_test_22, y_pred)
r2 = r2_score(y_test_22, y_pred)
print("\nmean_absolute_error of the model:",mae)
print("\nmean_squared_error of the model:",mse)
print("\nr2_score of the model:",r2)

# Fine-tune the model (RandomForestClassifier) with GridSearchCV
n_estimators_range = list(range(1, 31))

# Create a parameter grid: map the parameter names to the values that should be searched
param_grid = dict(n_estimators=n_estimators_range)

grid = GridSearchCV(RandomForestClassifier(max_depth=3, criterion='entropy'), param_grid, cv=10, scoring='accuracy')
grid.fit(x_train, y_train)

rfc=RandomForestClassifier(n_estimators=grid.best_params_['n_estimators'], max_depth=3, criterion='entropy')
rfc.fit(x_train,y_train)
y_pred=rfc.predict(x_test_22)
accuracy_score(y_pred,y_test_22)

# Evaluate the model
mae = mean_absolute_error(y_test_22, y_pred)
mse = mean_squared_error(y_test_22, y_pred)
r2 = r2_score(y_test_22, y_pred)

# Examine the best model
print("\ngrid.best_score:",grid.best_score_)
print("\ngrid.best_params:",grid.best_params_)
print("\ngrid.best_estimator:",grid.best_estimator_)
print("\nAccuracy of the best model:", accuracy_score(y_pred,y_test_22))
print("\nmean_absolute_error of the best model:",mae)
print("\nmean_squared_error of the best model:",mse)
print("\nr2_score of the best model:",r2)

# correlation between variables in selected_features and the target variable; overall_rating
for name, score in zip(x_train.columns, rfc.feature_importances_):
  print(name, score)

"""### Saving RandomForestRegressor model using pickle"""

filename = '/content/drive/My Drive/Colab Notebooks/Intro to AI Folder/Mid-Semester Project/player_rating_predictor.pkl'
pickle.dump(rf_model, open(filename, 'wb'))

loaded_model = pickle.load(open(filename, 'rb'))

y_pred = loaded_model.predict(x_test)

mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print("mean_absolute_error:",mae)
print("mean_squared_error:",mse)
print("r2_score:",r2)

if isinstance(loaded_model, RandomForestRegressor):
    print("The model is a RandomForestRegressor.")
else:
    print("The model is not a RandomForestRegressor.")